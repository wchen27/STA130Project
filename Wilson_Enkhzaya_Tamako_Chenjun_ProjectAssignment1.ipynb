{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Assignment 1\n",
    "Enkhzaya, Tamako, Wilson, Chenjun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_indicators = pd.read_csv('country_indicators.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.read_csv('test_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds['transformer_probability_prediction_error'] = np.abs(df_preds['y_true_transformer'].astype(float) - df_preds['y_pred_proba_transformer'])\n",
    "df_preds['xgboost_probability_prediction_error'] = np.abs(df_preds['y_true_xgboost'].astype(float) - df_preds['y_pred_proba_xgboost'])\n",
    "df_preds['ffnn_probability_prediction_error'] = np.abs(df_preds['y_true_ffnn'].astype(float) - df_preds['y_pred_proba_ffnn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40494034, 0.42866273])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating bootstrap confidence interval for ffnn\n",
    "\n",
    "np.random.seed(130)\n",
    "\n",
    "n, R = len(df_preds), 1000\n",
    "ffnn_bootstrap = []\n",
    "for i in range(R):\n",
    "    ffnn_bootstrap.append(df_preds.sample(n, replace=True)['ffnn_probability_prediction_error'].mean())\n",
    "\n",
    "np.percentile(ffnn_bootstrap, [2.5, 97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "df_preds['transformer_classification_performance_outcome'] = None\n",
    "\n",
    "tmp = df_preds['transformer_classification_performance_outcome'].copy()\n",
    "TP_pos_pred_correct = df_preds.y_true_transformer & (df_preds.y_pred_proba_transformer>threshold)\n",
    "tmp[TP_pos_pred_correct] = \"correct\"\n",
    "TN_neg_pred_correct = (~df_preds.y_true_transformer) & (df_preds.y_pred_proba_transformer<=threshold)\n",
    "tmp[TN_neg_pred_correct] = \"correct\"\n",
    "FP_pos_pred_wrong = (~df_preds.y_true_transformer) & (df_preds.y_pred_proba_transformer>threshold)\n",
    "tmp[FP_pos_pred_wrong] = \"incorrect\"\n",
    "FN_neg_pred_wrong = df_preds.y_true_transformer & (df_preds.y_pred_proba_transformer<=threshold)\n",
    "tmp[FN_neg_pred_wrong] = \"incorrect\"\n",
    "\n",
    "df_preds['transformer_classification_performance_outcome'] = tmp\n",
    "\n",
    "df_preds['xgboost_classification_performance_outcome'] = None\n",
    "\n",
    "tmp = df_preds['xgboost_classification_performance_outcome'].copy()\n",
    "TP_pos_pred_correct = df_preds.y_true_xgboost & (df_preds.y_pred_proba_xgboost>threshold)\n",
    "tmp[TP_pos_pred_correct] = \"correct\"\n",
    "TN_neg_pred_correct = (~df_preds.y_true_xgboost) & (df_preds.y_pred_proba_xgboost<=threshold)\n",
    "tmp[TN_neg_pred_correct] = \"correct\"\n",
    "FP_pos_pred_wrong = (~df_preds.y_true_xgboost) & (df_preds.y_pred_proba_xgboost>threshold)\n",
    "tmp[FP_pos_pred_wrong] = \"incorrect\"\n",
    "FN_neg_pred_wrong = df_preds.y_true_xgboost & (df_preds.y_pred_proba_xgboost<=threshold)\n",
    "tmp[FN_neg_pred_wrong] = \"incorrect\"\n",
    "\n",
    "df_preds['xgboost_classification_performance_outcome'] = tmp\n",
    "\n",
    "df_preds['ffnn_classification_performance_outcome'] = None\n",
    "\n",
    "tmp = df_preds['ffnn_classification_performance_outcome'].copy()\n",
    "TP_pos_pred_correct = df_preds.y_true_ffnn & (df_preds.y_pred_proba_ffnn>threshold)\n",
    "tmp[TP_pos_pred_correct] = \"correct\"\n",
    "TN_neg_pred_correct = (~df_preds.y_true_ffnn) & (df_preds.y_pred_proba_ffnn<=threshold)\n",
    "tmp[TN_neg_pred_correct] = \"correct\"\n",
    "FP_pos_pred_wrong = (~df_preds.y_true_ffnn) & (df_preds.y_pred_proba_ffnn>threshold)\n",
    "tmp[FP_pos_pred_wrong] = \"incorrect\"\n",
    "FN_neg_pred_wrong = df_preds.y_true_ffnn & (df_preds.y_pred_proba_ffnn<=threshold)\n",
    "tmp[FN_neg_pred_wrong] = \"incorrect\"\n",
    "\n",
    "df_preds['ffnn_classification_performance_outcome'] = tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating one-sample hypothesis test for ffnn \n",
    "\n",
    "H0 = 0.5\n",
    "\n",
    "sample = df_preds['ffnn_classification_performance_outcome'].copy()\n",
    "np.round(scipy.stats.ttest_1samp(sample == \"correct\", H0).pvalue, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_preds.merge(df_indicators, left_on='iso3', right_on='iso3', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our null hypothesis was $H_0: \\mu = 0.5$ and our alternative hypothesis was $H_1: \\mu \\neq 0.5$. \n",
    "Since p < 0.001, we have very strong evidence against the null hypothesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "warning = df[df['fsi_category'] == 'Warning']\n",
    "other = df[df['fsi_category'] != 'Warning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.001, 0.047])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating two sample bootstrap for warning and other for ffnn\n",
    "n1, n2, R = len(warning), len(other), 1000\n",
    "\n",
    "diffs = []\n",
    "\n",
    "for i in range(R):\n",
    "    warning_sample = warning.sample(n1, replace=True)['ffnn_probability_prediction_error'].mean()\n",
    "    other_sample = other.sample(n2, replace=True)['ffnn_probability_prediction_error'].mean()\n",
    "    diffs.append(warning_sample - other_sample)\n",
    "\n",
    "np.round(np.percentile(diffs, [2.5, 97.5]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.053"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating two sample hypothesis testing\n",
    "np.round(scipy.stats.ttest_ind(warning['ffnn_probability_prediction_error'], other['ffnn_probability_prediction_error']).pvalue, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our null hypothesis was $H_0: \\mu_{\\text{warning}} = \\mu_{\\text{others}}$ and our alternative hypothesis was $H_1: \\mu_{\\text{warning}} \\neq \\mu_{\\text{others}}$. Since our p-value was 0.053, we have weak evidence against the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05 , -0.008])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we compare the difference between ffnn and xgboost on all the data\n",
    "\n",
    "n, R = len(df_preds), 1000\n",
    "ffnn_xgboost_diffs = []\n",
    "\n",
    "for i in range(R):\n",
    "    sample = df_preds.sample(n, replace=True)\n",
    "    ffnn_sample = sample['ffnn_probability_prediction_error'].mean()\n",
    "    xgboost_sample = sample['xgboost_probability_prediction_error'].mean()\n",
    "    ffnn_xgboost_diffs.append(ffnn_sample - xgboost_sample)\n",
    "\n",
    "np.round(np.percentile(ffnn_xgboost_diffs, [2.5, 97.5]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating two sample hypothesis testing\n",
    "np.round(scipy.stats.ttest_rel(df_preds['ffnn_probability_prediction_error'], df_preds['xgboost_probability_prediction_error']).pvalue, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our null hypothesis was $H_0: \\mu_{\\text{ffnn}} = \\mu_{\\text{xgboost}}$ and our alternative hypothesis was $H_1: \\mu_{\\text{ffnn}} \\neq \\mu_{\\text{xgboost}}$. Since our p-value was 0.007, we have strong evidence against the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.137, -0.055])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two sample bootstrap on warning and other for ffnn and xgboost\n",
    "\n",
    "n1, n2, R = len(warning), len(other), 1000\n",
    "\n",
    "diffs = []\n",
    "\n",
    "for i in range(R):\n",
    "    curr_warning_sample = warning.sample(n1, replace=True)\n",
    "    curr_other_sample = other.sample(n2, replace=True)\n",
    "\n",
    "    ffnn_warning_sample = curr_warning_sample['ffnn_probability_prediction_error'].mean()\n",
    "    ffnn_other_sample = curr_other_sample['ffnn_probability_prediction_error'].mean()\n",
    "\n",
    "    xgboost_warning_sample = curr_warning_sample['xgboost_probability_prediction_error'].mean()\n",
    "    xgboost_other_sample = curr_other_sample['xgboost_probability_prediction_error'].mean()\n",
    "    \n",
    "    ffnn_diff = ffnn_warning_sample - ffnn_other_sample\n",
    "    xgboost_diff = xgboost_warning_sample - xgboost_other_sample\n",
    "\n",
    "    diffs.append(ffnn_diff - xgboost_diff)\n",
    "\n",
    "np.round(np.percentile(diffs, [2.5, 97.5]), 3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating two sample hypothesis testing for ffnn and xgboost on warning data\n",
    "np.round(scipy.stats.ttest_rel(warning['ffnn_probability_prediction_error'], warning['xgboost_probability_prediction_error']).pvalue, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our null hypothesis was $H_0: \\mu_{\\text{ffnn warning}} - \\mu_{\\text{xgboost warning}} = 0$ and our alternative hypothesis was $H_1: \\mu_{\\text{ffnn warning}} - \\mu_{\\text{xgboost warning}} \\neq 0$. Since our p-value was 0.000, we have very strong evidence against the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
